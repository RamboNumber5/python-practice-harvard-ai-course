{
 "cells": [
  {
   "cell_type": "code",
   "id": "3b20bff1011adfe2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T18:11:40.034759Z",
     "start_time": "2024-10-09T18:11:30.018735Z"
    }
   },
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"squad\")\n",
    "# print(dataset)\n",
    "\n",
    "def add_end_of_text(example):\n",
    "    example['question'] = example['question'] + '<|endoftext|> '\n",
    "    return example\n",
    "\n",
    "dataset = dataset.remove_columns(['id', 'context', 'answers', 'title'])\n",
    "dataset = dataset.map(add_end_of_text)\n",
    "\n",
    "dataset['train']['question'][:10]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?<|endoftext|> ',\n",
       " 'What is in front of the Notre Dame Main Building?<|endoftext|> ',\n",
       " 'The Basilica of the Sacred heart at Notre Dame is beside to which structure?<|endoftext|> ',\n",
       " 'What is the Grotto at Notre Dame?<|endoftext|> ',\n",
       " 'What sits on top of the Main Building at Notre Dame?<|endoftext|> ',\n",
       " 'When did the Scholastic Magazine of Notre dame begin publishing?<|endoftext|> ',\n",
       " \"How often is Notre Dame's the Juggler published?<|endoftext|> \",\n",
       " 'What is the daily student paper at Notre Dame called?<|endoftext|> ',\n",
       " 'How many student news papers are found at Notre Dame?<|endoftext|> ',\n",
       " 'In what year did the student paper Common Sense begin publication at Notre Dame?<|endoftext|> ']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T18:11:42.649701Z",
     "start_time": "2024-10-09T18:11:40.043868Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "model_checkpoint = \"distilgpt2\"\n",
    "# tokenizers are available in a python implementation of \"Fast\" implementation which uses the Rust language\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
    "\n",
    "# Example of tokenising\n",
    "sequence = (\"This tokenizer is being applied in CS197 at Harvard.<|endoftext|>\")"
   ],
   "id": "a93aba99abca58c1",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andym\\.conda\\envs\\python-practice-harvard-ai-course2\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T18:11:42.759743Z",
     "start_time": "2024-10-09T18:11:42.734662Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokens = tokenizer.tokenize(sequence)\n",
    "tokens"
   ],
   "id": "8baf5a8303ecdf28",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'Ġtoken',\n",
       " 'izer',\n",
       " 'Ġis',\n",
       " 'Ġbeing',\n",
       " 'Ġapplied',\n",
       " 'Ġin',\n",
       " 'ĠCS',\n",
       " '197',\n",
       " 'Ġat',\n",
       " 'ĠHarvard',\n",
       " '.',\n",
       " '<|endoftext|>']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T18:11:42.806885Z",
     "start_time": "2024-10-09T18:11:42.796694Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "ids"
   ],
   "id": "d10564d614a1c3bd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1212, 11241, 7509, 318, 852, 5625, 287, 9429, 24991, 379, 11131, 13, 50256]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-09T18:11:42.838706Z",
     "start_time": "2024-10-09T18:11:42.828708Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#the tokenizer actually automatically chains these operations for us when we use __call__:\n",
    "tokenizer(sequence)\n",
    "\n",
    "\n"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1212, 11241, 7509, 318, 852, 5625, 287, 9429, 24991, 379, 11131, 13, 50256], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T18:11:42.869552Z",
     "start_time": "2024-10-09T18:11:42.855431Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sequence2 = \"Hello my name is Andy and I'm just starting to use Jupyter Notebook\"\n",
    "tokens = tokenizer.tokenize(sequence2)\n",
    "tokens\n"
   ],
   "id": "2f7060b6e87e3d6a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Ġmy',\n",
       " 'Ġname',\n",
       " 'Ġis',\n",
       " 'ĠAndy',\n",
       " 'Ġand',\n",
       " 'ĠI',\n",
       " \"'m\",\n",
       " 'Ġjust',\n",
       " 'Ġstarting',\n",
       " 'Ġto',\n",
       " 'Ġuse',\n",
       " 'ĠJ',\n",
       " 'up',\n",
       " 'y',\n",
       " 'ter',\n",
       " 'ĠNote',\n",
       " 'book']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T18:11:43.249128Z",
     "start_time": "2024-10-09T18:11:42.886724Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import multiprocessing as mp\n",
    "# # Check if multiprocessing context is suitable\n",
    "# mp.set_start_method('spawn', force=True)\n",
    "# from multiprocess import set_start_method\n",
    "# set_start_method('spawn', force=True)\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"question\"], truncation=True)\n",
    "\n",
    "# By setting batched=True we process multiple elements of the dataset at once\n",
    "# num_proc sets the number of processes\n",
    "# Finally we remove the questions column because we won't need it now\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True, num_proc=1, remove_columns=[\"question\"])"
   ],
   "id": "fb3bec9f5aa3224c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/10570 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cbe023c4677346bb80a131c1e76416d1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T18:11:43.281129Z",
     "start_time": "2024-10-09T18:11:43.268130Z"
    }
   },
   "cell_type": "code",
   "source": "tokenized_datasets",
   "id": "dc182c2b740cca0f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 87599\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 10570\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T18:11:43.860920Z",
     "start_time": "2024-10-09T18:11:43.299130Z"
    }
   },
   "cell_type": "code",
   "source": [
    "block_size = 128\n",
    "\n",
    "def group_texts(examples):\n",
    "    # repeat concatenation fo rinput_ids and other keys\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    \n",
    "    #populae each of input_ids and other keys\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()        \n",
    "    }\n",
    "    \n",
    "    # add labels because we'll need it as the output\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "    \n",
    "lm_datasets = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    num_proc=1\n",
    ")"
   ],
   "id": "812048d251616b14",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/10570 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "439ffe44dcbd4062b888e7a01d3fdb97"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T18:11:44.346766Z",
     "start_time": "2024-10-09T18:11:43.878921Z"
    }
   },
   "cell_type": "code",
   "source": "print(lm_datasets['train']['input_ids'][0])",
   "id": "f54e0ba8cf87af9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2514, 4150, 750, 262, 5283, 5335, 7910, 1656, 287, 1248, 3365, 287, 406, 454, 8906, 4881, 30, 50256, 220, 2061, 318, 287, 2166, 286, 262, 23382, 20377, 8774, 11819, 30, 50256, 220, 464, 32520, 3970, 286, 262, 17380, 2612, 379, 23382, 20377, 318, 13970, 284, 543, 4645, 30, 50256, 220, 2061, 318, 262, 10299, 33955, 379, 23382, 20377, 30, 50256, 220, 2061, 10718, 319, 1353, 286, 262, 8774, 11819, 379, 23382, 20377, 30, 50256, 220, 2215, 750, 262, 3059, 349, 3477, 11175, 286, 23382, 288, 480, 2221, 12407, 30, 50256, 220, 2437, 1690, 318, 23382, 20377, 338, 262, 39296, 1754, 3199, 30, 50256, 220, 2061, 318, 262, 4445, 3710, 3348, 379, 23382, 20377, 1444, 30, 50256, 220, 2437, 867, 3710, 1705, 9473, 389, 1043, 379, 23382, 20377, 30]\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T18:11:44.819823Z",
     "start_time": "2024-10-09T18:11:44.364513Z"
    }
   },
   "cell_type": "code",
   "source": "print(tokenizer.decode(lm_datasets['train']['input_ids'][0]))",
   "id": "267949c0fa29dd0c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?<|endoftext|> What is in front of the Notre Dame Main Building?<|endoftext|> The Basilica of the Sacred heart at Notre Dame is beside to which structure?<|endoftext|> What is the Grotto at Notre Dame?<|endoftext|> What sits on top of the Main Building at Notre Dame?<|endoftext|> When did the Scholastic Magazine of Notre dame begin publishing?<|endoftext|> How often is Notre Dame's the Juggler published?<|endoftext|> What is the daily student paper at Notre Dame called?<|endoftext|> How many student news papers are found at Notre Dame?\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T18:11:44.866575Z",
     "start_time": "2024-10-09T18:11:44.836321Z"
    }
   },
   "cell_type": "code",
   "source": [
    "small_train_dataset = \\\n",
    "    lm_datasets[\"train\"].shuffle(seed=42).select(range(100))\n",
    "small_eval_dataset = \\\n",
    "    lm_datasets[\"validation\"].shuffle(seed=42).select(range(100))"
   ],
   "id": "dc49b0d7e7a4c003",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T18:35:04.664892Z",
     "start_time": "2024-10-09T18:31:53.824788Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n",
    "\n",
    "#Token removed again\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    f\"{model_checkpoint}-squad\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5, \n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model, \n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    ")\n",
    "\n",
    "# Not tried this but should increase the number of epochs from the default 3\n",
    "# Modify the number of epochs\n",
    "#trainer.args.num_train_epochs = 10\n",
    "\n",
    "trainer.train()"
   ],
   "id": "a8e4fbe716d1848f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='39' max='39' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [39/39 01:49, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.097046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.491851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.359954</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=39, training_loss=4.13212389823718, metrics={'train_runtime': 111.7001, 'train_samples_per_second': 2.686, 'train_steps_per_second': 0.349, 'total_flos': 9798628147200.0, 'train_loss': 4.13212389823718, 'epoch': 3.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T18:43:04.495037Z",
     "start_time": "2024-10-09T18:42:56.339701Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import math\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ],
   "id": "15caa34f89c22ad",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:07]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 28.79\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T18:48:18.558795Z",
     "start_time": "2024-10-09T18:47:11.154986Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer.save_pretrained('gpt2-squad')\n",
    "model.push_to_hub('gpt2-squad')"
   ],
   "id": "bea668f61be0643f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/328M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "021b9d486f8a43bdbd36b1c11bc0e5f7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/AndyRoberts/gpt2-squad/commit/f33753b8418f6f44a15cc301eb7ea51fd3d09412', commit_message='Upload model', commit_description='', oid='f33753b8418f6f44a15cc301eb7ea51fd3d09412', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c3010dde7ae1f814"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
